{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walmart Recruiting II: Sales in Stormy Weather With Spark\n",
    "\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "\n",
    "\n",
    "\n",
    "This notebook will detail the same methods covered in the walmart_sales_forecasting notebook, but instead leveraging Spark to generate features and perform modeling.\n",
    "\n",
    "# Notebook Contents\n",
    "## Part I: Feature Generation\n",
    "## Part II: Model Testing and Optimization\n",
    "## Part III: Transforming Test Data\n",
    "## Part IV: Generating Predictions\n",
    "## Part V: Discussion of Results\n",
    "\n",
    "# Part I: Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import calendar\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import datediff, to_date, lit, date_format\n",
    "import datetime\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Walmart Pyspark Implementation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Walmart Pyspark Implementation>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local[*]\") \\\n",
    "   .appName(\"Walmart Pyspark Implementation\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- store_nbr: string (nullable = true)\n",
      " |-- item_nbr: string (nullable = true)\n",
      " |-- units: string (nullable = true)\n",
      "\n",
      "None\n",
      "+----------+---------+--------+-----+\n",
      "|      date|store_nbr|item_nbr|units|\n",
      "+----------+---------+--------+-----+\n",
      "|2012-01-01|        1|       1|    0|\n",
      "|2012-01-01|        1|       2|    0|\n",
      "|2012-01-01|        1|       3|    0|\n",
      "|2012-01-01|        1|       4|    0|\n",
      "|2012-01-01|        1|       5|    0|\n",
      "|2012-01-01|        1|       6|    0|\n",
      "|2012-01-01|        1|       7|    0|\n",
      "|2012-01-01|        1|       8|    0|\n",
      "|2012-01-01|        1|       9|   29|\n",
      "|2012-01-01|        1|      10|    0|\n",
      "|2012-01-01|        1|      11|    0|\n",
      "|2012-01-01|        1|      12|    0|\n",
      "|2012-01-01|        1|      13|    0|\n",
      "|2012-01-01|        1|      14|    0|\n",
      "|2012-01-01|        1|      15|    0|\n",
      "|2012-01-01|        1|      16|    0|\n",
      "|2012-01-01|        1|      17|    0|\n",
      "|2012-01-01|        1|      18|    0|\n",
      "|2012-01-01|        1|      19|    0|\n",
      "|2012-01-01|        1|      20|    0|\n",
      "+----------+---------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv(\"datasets/train.csv\",header=True)\n",
    "print df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year, month, day\n",
    "split_col = pyspark.sql.functions.split(df['date'], '-')\n",
    "df = df.withColumn('year', split_col.getItem(0))\n",
    "df = df.withColumn('month', split_col.getItem(1))\n",
    "df = df.withColumn('day', split_col.getItem(2))\n",
    "# extract day of year\n",
    "df = df.withColumn('day_of_year', pyspark.sql.functions.dayofyear(df['date']))\n",
    "# extract day of cycle\n",
    "first_date=df.agg({\"date\": \"min\"}).collect()[0][0]\n",
    "df = df.withColumn(\"day_of_cycle\", \n",
    "              datediff(to_date(\"date\",\"yyyy-MM-dd\"),to_date(lit(first_date))))\n",
    "# extract day of week\n",
    "df=df.withColumn(\"day_of_week\", \n",
    "              date_format('date', 'u').alias('day_of_week'))\n",
    "# extract weekend\n",
    "df=df.withColumn(\"is_weekend\", \\\n",
    "                 pyspark.sql.functions.when(df.day_of_week == 7, 1).when(df.day_of_week == 6, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of holidays\n",
    "cal = USFederalHolidayCalendar()\n",
    "more_holidays=['01/01/2012','04/08/2012','03/31/2013','04/20/2014','12/24/2012','12/24/2013','12/24/2014']\n",
    "holidays_df=pd.concat([cal.holidays(start='2011-12-31', end='2014-12-31').to_frame().reset_index(drop=True),pd.DataFrame(more_holidays)]).apply(lambda x: pd.to_datetime(x)).reset_index(drop=True)\n",
    "holidays_df=holidays_df.iloc[1:,:].sort_values([0]).reset_index(drop=True)\n",
    "holidays_set=set(holidays_df[0].apply(lambda x: x.strftime('%Y-%m-%d')).tolist())\n",
    "\n",
    "# is holiday\n",
    "df=df.withColumn(\"is_holiday\", \\\n",
    "                 pyspark.sql.functions.when(df.date.isin(holidays_set), 1).otherwise(0))\n",
    "\n",
    "#within one day of holiday but not on holiday\n",
    "holiday_1day=set(pd.concat([holidays_df.apply(lambda x: x + datetime.timedelta(days=1)),\\\n",
    "                            holidays_df.apply(lambda x: x + datetime.timedelta(days=-1))])[0]\\\n",
    "                 .apply(lambda x: x.strftime('%Y-%m-%d')).tolist())\n",
    "\n",
    "df=df.withColumn(\"1d_away_holiday\", \\\n",
    "                 pyspark.sql.functions.when(df.date.isin(holiday_1day), 1).otherwise(0))\n",
    "\n",
    "#within two days of holiday but not on holiday\n",
    "holiday_1day.update(pd.concat([holidays_df.apply(lambda x: x + datetime.timedelta(days=2)),\\\n",
    "                               holidays_df.apply(lambda x: x + datetime.timedelta(days=-2))])[0]\\\n",
    "                    .apply(lambda x: x.strftime('%Y-%m-%d')).tolist())\n",
    "\n",
    "df=df.withColumn(\"2d_away_holiday\", \\\n",
    "                 pyspark.sql.functions.when(df.date.isin(holiday_1day), 1).otherwise(0))\n",
    "\n",
    "#within three days of holiday but not on holiday\n",
    "holiday_1day.update(pd.concat([holidays_df.apply(lambda x: x + datetime.timedelta(days=3)),\\\n",
    "                               holidays_df.apply(lambda x: x + datetime.timedelta(days=-3))])[0]\\\n",
    "                    .apply(lambda x: x.strftime('%Y-%m-%d')).tolist())\n",
    "\n",
    "df=df.withColumn(\"3d_away_holiday\", \\\n",
    "                 pyspark.sql.functions.when(df.date.isin(holiday_1day), 1).otherwise(0))\n",
    "\n",
    "#within seven days of holiday but not on holiday\n",
    "holiday_1day.update(pd.concat([holidays_df.apply(lambda x: x + datetime.timedelta(days=7)),\\\n",
    "                               holidays_df.apply(lambda x: x + datetime.timedelta(days=-7))])[0]\\\n",
    "                    .apply(lambda x: x.strftime('%Y-%m-%d')).tolist())\n",
    "\n",
    "df=df.withColumn(\"7d_away_holiday\", \\\n",
    "                 pyspark.sql.functions.when(df.date.isin(holiday_1day), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe types after transform\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('date', 'string'),\n",
       " ('store_nbr', 'string'),\n",
       " ('item_nbr', 'string'),\n",
       " ('units', 'float'),\n",
       " ('year', 'float'),\n",
       " ('month', 'float'),\n",
       " ('day', 'float'),\n",
       " ('day_of_year', 'float'),\n",
       " ('day_of_cycle', 'float'),\n",
       " ('day_of_week', 'float'),\n",
       " ('is_weekend', 'string'),\n",
       " ('is_holiday', 'string'),\n",
       " ('1d_away_holiday', 'string'),\n",
       " ('2d_away_holiday', 'string'),\n",
       " ('3d_away_holiday', 'string'),\n",
       " ('7d_away_holiday', 'string'),\n",
       " ('units_log', 'double'),\n",
       " ('store_nbr_indexed', 'double'),\n",
       " ('item_nbr_indexed', 'double'),\n",
       " ('is_weekend_indexed', 'double'),\n",
       " ('is_holiday_indexed', 'double'),\n",
       " ('1d_away_holiday_indexed', 'double'),\n",
       " ('2d_away_holiday_indexed', 'double'),\n",
       " ('3d_away_holiday_indexed', 'double'),\n",
       " ('7d_away_holiday_indexed', 'double'),\n",
       " ('store_nbr_indexed_encoded', 'double'),\n",
       " ('item_nbr_indexed_encoded', 'double'),\n",
       " ('is_weekend_indexed_encoded', 'double'),\n",
       " ('is_holiday_indexed_encoded', 'double'),\n",
       " ('1d_away_holiday_indexed_encoded', 'double'),\n",
       " ('2d_away_holiday_indexed_encoded', 'double'),\n",
       " ('3d_away_holiday_indexed_encoded', 'double'),\n",
       " ('7d_away_holiday_indexed_encoded', 'double'),\n",
       " ('cat', 'vector'),\n",
       " ('num', 'vector')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert numerical columns to numerical and categorical to string\n",
    "# Import all from `sql.types`\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "cat_cols = [\"store_nbr\", \"item_nbr\",'is_weekend','is_holiday','1d_away_holiday','2d_away_holiday','3d_away_holiday','7d_away_holiday']\n",
    "num_cols = [\"year\", \"month\", \"day\", \"day_of_year\",'day_of_cycle','day_of_week']\n",
    "target_feat = ['units']\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df, num_cols, FloatType())\n",
    "df = convertColumn(df, cat_cols, StringType())\n",
    "df = convertColumn(df, target_feat, FloatType())\n",
    "\n",
    "# log transform target var: units\n",
    "df=df.withColumn(\"units_log\", \\\n",
    "              pyspark.sql.functions.log(df.units+1))\n",
    "\n",
    "print \"dataframe types after transform\"\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|units|         units_log|\n",
      "+-----+------------------+\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "| 29.0|3.4011973816621555|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "|  0.0|               0.0|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ensure that transformation was successful\n",
    "df.select('units','units_log').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/49853188/create-features-column-in-pyspark-with-both-numerical-and-categorical-variable?noredirect=1&lq=1\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# categorical pipeline\n",
    "indexers = [StringIndexer(inputCol = c, outputCol=\"{0}_indexed\".format(c)) for c in cat_cols]\n",
    "\n",
    "encoders = [StringIndexer(inputCol = indexer.getOutputCol(), outputCol = \"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "for indexer in indexers]\n",
    "\n",
    "assemblerCat = VectorAssembler(inputCols = [encoder.getOutputCol() for encoder in encoders], outputCol = \"cat\")\n",
    "\n",
    "pipelineCat = Pipeline(stages = indexers + encoders + [assemblerCat])\n",
    "df = pipelineCat.fit(df).transform(df)\n",
    "\n",
    "#numerical pipeline\n",
    "assemblerNum = VectorAssembler(inputCols = num_cols, outputCol = \"num\")\n",
    "\n",
    "pipelineNum = Pipeline(stages = [assemblerNum])\n",
    "df = pipelineNum.fit(df).transform(df)\n",
    "\n",
    "assembler = VectorAssembler(inputCols = [\"cat\", \"num\"], outputCol = \"features_not_scaled\")\n",
    "\n",
    "pipeline = Pipeline(stages = [assembler])\n",
    "df_final = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"num\", outputCol=\"num_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df_final)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df_final)\n",
    "\n",
    "assembler = VectorAssembler(inputCols = [\"cat\", \"num_scaled\"], outputCol = \"features\")\n",
    "\n",
    "pipeline = Pipeline(stages = [assembler])\n",
    "df_complete = pipeline.fit(scaled_df).transform(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|units_log|            features|\n",
      "+---------+--------------------+\n",
      "|      0.0|[18.0,52.0,1.0,1....|\n",
      "+---------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assemble final training vector\n",
    "# Re-order and select columns\n",
    "df_complete1 = df_complete.select(\"units_log\", 'features')\n",
    "df_complete1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(units_log=0.0, features=DenseVector([18.0, 52.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2499.6846, 0.2956, 0.114, 0.0097, 0.0, 3.4973]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete1.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Model Testing and Optimization\n",
    "A linear regression model will be tested against a random forest model. The winning model of the previous notebook returned a 0.435944 RMSE score on the training set, so that will be the standard that these models will be compared with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(featuresCol=\"features\",labelCol=\"units_log\", predictionCol=\"units_pred\", maxIter=10, regParam=0.3)\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.2,0.4,0.5]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"units_log\", predictionCol=\"units_pred\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "#model=crossval.fit(df_complete1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.10077853022075889, 0.0),\n",
       " (0.1009473193743311, 0.0),\n",
       " (0.0992823995591543, 0.0),\n",
       " (0.10093760265621032, 0.0),\n",
       " (0.10078321560223102, 0.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = df_complete1.randomSplit([.8,.2],seed=1234)\n",
    "\n",
    "model1=crossval.fit(train_data)\n",
    "\n",
    "predicted = model1.bestModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"units_pred\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"units_log\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5135913940449095"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.bestModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_complete1.randomSplit([.8,.2],seed=1234)\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",labelCol=\"units_log\", \\\n",
    "                           predictionCol=\"units_pred_rf\", maxDepth=10, numTrees=20,maxBins=120)\n",
    "\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10,30,40]) \\\n",
    "    .addGrid(rf.maxDepth, [5,10]) \\\n",
    "    .build()\n",
    "\n",
    "crossval_rf = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid_rf,\n",
    "                          evaluator= RegressionEvaluator(labelCol=\"units_log\", predictionCol=\"units_pred_rf\"),\n",
    "                          numFolds=5)\n",
    "\n",
    "model_rf=crossval_rf.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.004809465615159759, 0.0),\n",
       " (0.010967348988976163, 0.0),\n",
       " (0.004693017123876308, 0.0),\n",
       " (0.007607799472912463, 0.0),\n",
       " (0.009039187172225157, 0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_rf = model_rf.bestModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions_rf = predicted_rf.select(\"units_pred_rf\").rdd.map(lambda x: x[0])\n",
    "labels_rf = predicted_rf.select(\"units_log\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel_rf = predictions_rf.zip(labels_rf).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel_rf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on training set = 0.284088\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"units_log\", predictionCol=\"units_pred_rf\", metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predicted_rf)\n",
    "print(\"Root Mean Squared Error (RMSE) on training set = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the RandomForestRegressor model are promising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Transforming Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- store_nbr: string (nullable = true)\n",
      " |-- item_nbr: string (nullable = true)\n",
      "\n",
      "None\n",
      "+----------+---------+--------+\n",
      "|      date|store_nbr|item_nbr|\n",
      "+----------+---------+--------+\n",
      "|2013-04-01|        2|       1|\n",
      "|2013-04-01|        2|       2|\n",
      "|2013-04-01|        2|       3|\n",
      "|2013-04-01|        2|       4|\n",
      "|2013-04-01|        2|       5|\n",
      "|2013-04-01|        2|       6|\n",
      "|2013-04-01|        2|       7|\n",
      "|2013-04-01|        2|       8|\n",
      "|2013-04-01|        2|       9|\n",
      "|2013-04-01|        2|      10|\n",
      "|2013-04-01|        2|      11|\n",
      "|2013-04-01|        2|      12|\n",
      "|2013-04-01|        2|      13|\n",
      "|2013-04-01|        2|      14|\n",
      "|2013-04-01|        2|      15|\n",
      "|2013-04-01|        2|      16|\n",
      "|2013-04-01|        2|      17|\n",
      "|2013-04-01|        2|      18|\n",
      "|2013-04-01|        2|      19|\n",
      "|2013-04-01|        2|      20|\n",
      "+----------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df=spark.read.csv(\"datasets/test.csv\",header=True)\n",
    "sub_df=spark.read.csv(\"datasets/sampleSubmission.csv\",header=True)\n",
    "print test_df.printSchema()\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|            id|units|\n",
      "+--------------+-----+\n",
      "|2_1_2013-04-01|    0|\n",
      "|2_2_2013-04-01|    0|\n",
      "|2_3_2013-04-01|    0|\n",
      "+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year, month, day\n",
    "split_col = pyspark.sql.functions.split(test_df['date'], '-')\n",
    "test_df = test_df.withColumn('year', split_col.getItem(0))\n",
    "test_df = test_df.withColumn('month', split_col.getItem(1))\n",
    "test_df = test_df.withColumn('day', split_col.getItem(2))\n",
    "# extract day of year\n",
    "test_df = test_df.withColumn('day_of_year', pyspark.sql.functions.dayofyear(test_df['date']))\n",
    "# extract day of cycle\n",
    "first_date=test_df.agg({\"date\": \"min\"}).collect()[0][0]\n",
    "test_df = test_df.withColumn(\"day_of_cycle\", \n",
    "              datediff(to_date(\"date\",\"yyyy-MM-dd\"),to_date(lit(first_date))))\n",
    "# extract day of week\n",
    "test_df=test_df.withColumn(\"day_of_week\", \n",
    "              date_format('date', 'u').alias('day_of_week'))\n",
    "# extract weekend\n",
    "test_df=test_df.withColumn(\"is_weekend\", \\\n",
    "                 pyspark.sql.functions.when(test_df.day_of_week == 7, 1).when(test_df.day_of_week == 6, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of holidays\n",
    "cal = USFederalHolidayCalendar()\n",
    "more_holidays=['01/01/2012','04/08/2012','03/31/2013','04/20/2014','12/24/2012','12/24/2013','12/24/2014']\n",
    "holidays_df=pd.concat([cal.holidays(start='2011-12-31', end='2014-12-31').to_frame().reset_index(drop=True),pd.DataFrame(more_holidays)]).apply(lambda x: pd.to_datetime(x)).reset_index(drop=True)\n",
    "holidays_df=holidays_df.iloc[1:,:].sort_values([0]).reset_index(drop=True)\n",
    "holidays_set=set(holidays_df[0].apply(lambda x: x.strftime('%Y-%m-%d')).tolist())\n",
    "\n",
    "\n",
    "# is holiday\n",
    "test_df=test_df.withColumn(\"is_holiday\", \\\n",
    "                 pyspark.sql.functions.when(test_df.date.isin(holidays_set), 1).otherwise(0))\n",
    "\n",
    "#within one day of holiday but not on holiday\n",
    "holiday_1day=set(pd.concat([holidays_df.apply(lambda x: x + datetime.timedelta(days=1)),\\\n",
    "                            holidays_df.apply(lambda x: x + datetime.timedelta(days=-1))])[0]\\\n",
    "                 .apply(lambda x: x.strftime('%Y-%m-%d')).tolist())\n",
    "\n",
    "test_df=test_df.withColumn(\"1d_away_holiday\", \\\n",
    "                 pyspark.sql.functions.when(test_df.date.isin(holiday_1day), 1).otherwise(0))\n",
    "\n",
    "#within two days of holiday but not on holiday\n",
    "holiday_1day.update(pd.concat([holidays_df.apply(lambda x: x + datetime.timedelta(days=2)),\\\n",
    "                               holidays_df.apply(lambda x: x + datetime.timedelta(days=-2))])[0]\\\n",
    "                    .apply(lambda x: x.strftime('%Y-%m-%d')).tolist())\n",
    "\n",
    "test_df=test_df.withColumn(\"2d_away_holiday\", \\\n",
    "                 pyspark.sql.functions.when(test_df.date.isin(holiday_1day), 1).otherwise(0))\n",
    "\n",
    "#within three days of holiday but not on holiday\n",
    "holiday_1day.update(pd.concat([holidays_df.apply(lambda x: x + datetime.timedelta(days=3)),\\\n",
    "                               holidays_df.apply(lambda x: x + datetime.timedelta(days=-3))])[0]\\\n",
    "                    .apply(lambda x: x.strftime('%Y-%m-%d')).tolist())\n",
    "\n",
    "test_df=test_df.withColumn(\"3d_away_holiday\", \\\n",
    "                 pyspark.sql.functions.when(test_df.date.isin(holiday_1day), 1).otherwise(0))\n",
    "\n",
    "#within seven days of holiday but not on holiday\n",
    "holiday_1day.update(pd.concat([holidays_df.apply(lambda x: x + datetime.timedelta(days=7)),\\\n",
    "                               holidays_df.apply(lambda x: x + datetime.timedelta(days=-7))])[0]\\\n",
    "                    .apply(lambda x: x.strftime('%Y-%m-%d')).tolist())\n",
    "\n",
    "test_df=test_df.withColumn(\"7d_away_holiday\", \\\n",
    "                 pyspark.sql.functions.when(test_df.date.isin(holiday_1day), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe types after transform\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('date', 'string'),\n",
       " ('store_nbr', 'string'),\n",
       " ('item_nbr', 'string'),\n",
       " ('year', 'float'),\n",
       " ('month', 'float'),\n",
       " ('day', 'float'),\n",
       " ('day_of_year', 'float'),\n",
       " ('day_of_cycle', 'float'),\n",
       " ('day_of_week', 'float'),\n",
       " ('is_weekend', 'string'),\n",
       " ('is_holiday', 'string'),\n",
       " ('1d_away_holiday', 'string'),\n",
       " ('2d_away_holiday', 'string'),\n",
       " ('3d_away_holiday', 'string'),\n",
       " ('7d_away_holiday', 'string')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert numerical columns to numerical and categorical to string\n",
    "# Import all from `sql.types`\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(test_df, names, newType):\n",
    "  for name in names: \n",
    "     test_df = test_df.withColumn(name, test_df[name].cast(newType))\n",
    "  return test_df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "cat_cols = [\"store_nbr\", \"item_nbr\",'is_weekend','is_holiday','1d_away_holiday','2d_away_holiday','3d_away_holiday','7d_away_holiday']\n",
    "num_cols = [\"year\", \"month\", \"day\", \"day_of_year\",'day_of_cycle','day_of_week']\n",
    "\n",
    "# Conver the `test_df` columns to `FloatType()`\n",
    "test_df = convertColumn(test_df, num_cols, FloatType())\n",
    "test_df = convertColumn(test_df, cat_cols, StringType())\n",
    "\n",
    "print \"dataframe types after transform\"\n",
    "test_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date=u'2013-04-01', store_nbr=u'2', item_nbr=u'1', year=2013.0, month=4.0, day=1.0, day_of_year=91.0, day_of_cycle=0.0, day_of_week=1.0, is_weekend=u'0', is_holiday=u'0', 1d_away_holiday=u'1', 2d_away_holiday=u'1', 3d_away_holiday=u'1', 7d_away_holiday=u'1', store_nbr_indexed=4.0, item_nbr_indexed=69.0, is_weekend_indexed=0.0, is_holiday_indexed=0.0, 1d_away_holiday_indexed=1.0, 2d_away_holiday_indexed=1.0, 3d_away_holiday_indexed=1.0, 7d_away_holiday_indexed=1.0, store_nbr_indexed_encoded=4.0, item_nbr_indexed_encoded=52.0, is_weekend_indexed_encoded=0.0, is_holiday_indexed_encoded=0.0, 1d_away_holiday_indexed_encoded=1.0, 2d_away_holiday_indexed_encoded=1.0, 3d_away_holiday_indexed_encoded=1.0, 7d_away_holiday_indexed_encoded=1.0, cat=DenseVector([4.0, 52.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]), num=DenseVector([2013.0, 4.0, 1.0, 91.0, 0.0, 1.0]), features_not_scaled=DenseVector([4.0, 52.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2013.0, 4.0, 1.0, 91.0, 0.0, 1.0])),\n",
       " Row(date=u'2013-04-01', store_nbr=u'2', item_nbr=u'2', year=2013.0, month=4.0, day=1.0, day_of_year=91.0, day_of_cycle=0.0, day_of_week=1.0, is_weekend=u'0', is_holiday=u'0', 1d_away_holiday=u'1', 2d_away_holiday=u'1', 3d_away_holiday=u'1', 7d_away_holiday=u'1', store_nbr_indexed=4.0, item_nbr_indexed=91.0, is_weekend_indexed=0.0, is_holiday_indexed=0.0, 1d_away_holiday_indexed=1.0, 2d_away_holiday_indexed=1.0, 3d_away_holiday_indexed=1.0, 7d_away_holiday_indexed=1.0, store_nbr_indexed_encoded=4.0, item_nbr_indexed_encoded=73.0, is_weekend_indexed_encoded=0.0, is_holiday_indexed_encoded=0.0, 1d_away_holiday_indexed_encoded=1.0, 2d_away_holiday_indexed_encoded=1.0, 3d_away_holiday_indexed_encoded=1.0, 7d_away_holiday_indexed_encoded=1.0, cat=DenseVector([4.0, 73.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]), num=DenseVector([2013.0, 4.0, 1.0, 91.0, 0.0, 1.0]), features_not_scaled=DenseVector([4.0, 73.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2013.0, 4.0, 1.0, 91.0, 0.0, 1.0]))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/49853188/create-features-column-in-pyspark-with-both-numerical-and-categorical-variable?noredirect=1&lq=1\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# categorical pipeline\n",
    "indexers = [StringIndexer(inputCol = c, outputCol=\"{0}_indexed\".format(c)) for c in cat_cols]\n",
    "\n",
    "encoders = [StringIndexer(inputCol = indexer.getOutputCol(), outputCol = \"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "for indexer in indexers]\n",
    "\n",
    "assemblerCat = VectorAssembler(inputCols = [encoder.getOutputCol() for encoder in encoders], outputCol = \"cat\")\n",
    "\n",
    "pipelineCat = Pipeline(stages = indexers + encoders + [assemblerCat])\n",
    "test_df = pipelineCat.fit(test_df).transform(test_df)\n",
    "\n",
    "#numerical pipeline\n",
    "assemblerNum = VectorAssembler(inputCols = num_cols, outputCol = \"num\")\n",
    "\n",
    "pipelineNum = Pipeline(stages = [assemblerNum])\n",
    "test_df = pipelineNum.fit(test_df).transform(test_df)\n",
    "\n",
    "assembler = VectorAssembler(inputCols = [\"cat\", \"num\"], outputCol = \"features_not_scaled\")\n",
    "\n",
    "pipeline = Pipeline(stages = [assembler])\n",
    "test_df = pipeline.fit(test_df).transform(test_df)\n",
    "test_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"num\", outputCol=\"num_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler_test = standardScaler.fit(test_df)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "test_df = scaler_test.transform(test_df)\n",
    "\n",
    "assembler = VectorAssembler(inputCols = [\"cat\", \"num_scaled\"], outputCol = \"features\")\n",
    "\n",
    "pipeline = Pipeline(stages = [assembler])\n",
    "test_df = pipeline.fit(test_df).transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[4.0,52.0,0.0,0.0...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_complete = test_df.select('features')\n",
    "test_complete.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            features|       units_pred_rf|        actual_units|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[4.0,52.0,0.0,0.0...|0.009658993904179295|0.009705792540354115|\n",
      "|[4.0,73.0,0.0,0.0...|0.031625026901596034|0.032130411596397096|\n",
      "|[4.0,106.0,0.0,0....|0.009658993904179295|0.009705792540354115|\n",
      "|[4.0,75.0,0.0,0.0...|0.009139632375107622|0.009181526349657524|\n",
      "|[4.0,32.0,0.0,0.0...|   3.166398091154888|   22.72188621883388|\n",
      "|[4.0,37.0,0.0,0.0...|0.005222504277240718|0.005236165323975045|\n",
      "|[4.0,2.0,0.0,0.0,...|0.009658993904179295|0.009705792540354115|\n",
      "|[4.0,50.0,0.0,0.0...|0.006321803590176853|0.006341828365819665|\n",
      "|[4.0,88.0,0.0,0.0...|  0.9217504015089155|   1.513686501979287|\n",
      "|[4.0,53.0,0.0,0.0...|0.014041481872560522|0.014140526514044726|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predicted_rf = model_rf.bestModel.transform(test_complete)\n",
    "test_predicted_rf = test_predicted_rf.withColumn(\"actual_units\", pyspark.sql.functions.expm1(test_predicted_rf.units_pred_rf))\n",
    "test_predicted_rf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of ids and matching ids\n",
    "test_predictions=test_predicted_rf.select('actual_units').rdd.flatMap(lambda x: x).collect()\n",
    "sub_ids=sub_df.select('id').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "test_predictions_export_df=spark.createDataFrame(zip(sub_ids,test_predictions) , [\"id\", \"units\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|             id|               units|\n",
      "+---------------+--------------------+\n",
      "| 2_1_2013-04-01|0.009705792540354115|\n",
      "| 2_2_2013-04-01|0.032130411596397096|\n",
      "| 2_3_2013-04-01|0.009705792540354115|\n",
      "| 2_4_2013-04-01|0.009181526349657524|\n",
      "| 2_5_2013-04-01|   22.72188621883388|\n",
      "| 2_6_2013-04-01|0.005236165323975045|\n",
      "| 2_7_2013-04-01|0.009705792540354115|\n",
      "| 2_8_2013-04-01|0.006341828365819665|\n",
      "| 2_9_2013-04-01|   1.513686501979287|\n",
      "|2_10_2013-04-01|0.014140526514044726|\n",
      "|2_11_2013-04-01|0.007437953008162322|\n",
      "|2_12_2013-04-01|0.014097478746625642|\n",
      "|2_13_2013-04-01|0.009705792540354115|\n",
      "|2_14_2013-04-01|0.009181526349657524|\n",
      "|2_15_2013-04-01|  0.1291213063078504|\n",
      "|2_16_2013-04-01|0.013961469509229968|\n",
      "|2_17_2013-04-01| 0.07204212757354697|\n",
      "|2_18_2013-04-01|0.007437953008162322|\n",
      "|2_19_2013-04-01|0.009181526349657524|\n",
      "|2_20_2013-04-01|0.009705792540354115|\n",
      "+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions_export_df.toPandas().to_csv('submissions/pyspark_sub.csv',index=False)\n",
    "test_predictions_export_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>526917.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.315809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.012295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.005566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.009217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.017443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>102.732760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               units\n",
       "count  526917.000000\n",
       "mean        0.315809\n",
       "std         3.012295\n",
       "min         0.000000\n",
       "25%         0.005566\n",
       "50%         0.009217\n",
       "75%         0.017443\n",
       "max       102.732760"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('submissions/pyspark_sub.csv').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Discussion of Results\n",
    "When uploaded to kaggle, the random forest model returns a 0.51073 RMSLE score, worse compared to the sub .10 RMSLE score achieved with xgboost. However, a caveat of using the pyspark technique is that I used the entire dataset to generate predictions on the test data, whereas the xgboost score was achieved by generating predictions on only items that had historical sales and filling 0 sales for items that had none. This approach was taken as training on the entire dataset led to memory capacity errors, which is why Spark is favored to handling massive datasets. I suspect that if most of the dataset were not 0's, the Spark approach would beat out the traditional approach levearging pandas and sklearn libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
